{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence to Sequence Models for Convertational Agents\n",
    "\n",
    "Implementation of encoder decoder architechtures in Python3 with Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "Dataset MetaLWoz: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Maximum and Minimum Sentence length\n",
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility for flattening iterables\n",
    "FLATTEN = lambda l: [i for e in l for i in e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JSON Parse and prepend greeting\n",
    "def read_file(filename):\n",
    "    return [['Hi Mr Robot'] + json.loads(line)['turns'] for line in open(filename, 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Data is ordered in dialogues creates pairs from a single dialogue\n",
    "def create_pairs_from_dialogue(dialogue):\n",
    "    return [(dialogue[2 * i], dialogue[2 * i + 1]) for i in range(len(dialogue) // 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Wrapper to previous function to create all pairs\n",
    "def create_pairs(dialogues):\n",
    "    return FLATTEN(map(create_pairs_from_dialogue, dialogues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Apply length constraints\n",
    "def filter_pairs(pairs):\n",
    "    length_constraints = lambda s: MIN_LENGTH <= len(s.split()) <= MAX_LENGTH\n",
    "    filter_pair = lambda p: all(length_constraints(s) for s in p)\n",
    "    return [*filter(filter_pair, pairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove punctuation and convert to lowercase\n",
    "def normalize_sentence(sentence):\n",
    "    return sentence.translate({ord(i): None for i in string.punctuation}).lower()\n",
    "\n",
    "### Wrapper for normalize_sentence\n",
    "def normalize_pairs(pairs):\n",
    "    return [*map(lambda p: tuple(map(normalize_sentence, p)), pairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a dictionary of all unique words and SOS and EOS tokens\n",
    "def create_dictionary(pairs):\n",
    "    all_words = FLATTEN([q.split() + a.split() for q, a in pairs])\n",
    "    unique_words = set(all_words)\n",
    "    dictionary = {key: value + 2 for value, key in enumerate(unique_words)}\n",
    "    return {**{'SOS': 0, 'EOS': 1}, **dictionary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encodes sentence from dictionary\n",
    "def encode_sentence(s, dictionary):\n",
    "    return [dictionary[word] for word in s.split()]\n",
    "\n",
    "### Wrapper for encode_sentence\n",
    "def encode_pairs(pairs, dictionary):\n",
    "    return [*map(lambda p: tuple(map(lambda s: encode_sentence(s, dictionary), p)), pairs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pairs(filename):\n",
    "    dialogues = read_file(filename)\n",
    "    pairs = create_pairs(dialogues)\n",
    "    filtered_pairs = filter_pairs(pairs)\n",
    "    normalized_pairs = normalize_pairs(filtered_pairs)\n",
    "    return normalized_pairs\n",
    "\n",
    "def create_dictionary_encode(pairs):\n",
    "    dictionary = create_dictionary(pairs)\n",
    "    encoded_pairs = encode_pairs(pairs, dictionary)\n",
    "    append_EOS = lambda p: (p[0], p[1] + [1])\n",
    "    return [*map(append_EOS, encoded_pairs)], dictionary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Function\n",
    "def reverse_dictionary(dictionary):\n",
    "    return {value: key for key, value in dictionary.items()}\n",
    "\n",
    "def decode_sentence(s, dictionary):\n",
    "    return [dictionary[w] for w in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Encoders and Decoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encoder architechture includes an embedding layer to tranform words to a constant size vector. Otherwise, we would have to process words an one hot vectors of size equal to the size of the vocabulary which would be sparse and challenging to learn from.\n",
    "\n",
    "Documentation for layer used here: https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, x, s):\n",
    "        embedded = self.embedding(x)\n",
    "        return self.gru(embedded, s)\n",
    "    \n",
    "    def __init_hidden__(self, hidden_size): return torch.zeros(1, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoder has a similar architechture to the encoder but since output do matter in this case we map the hidden state to a vector of size equal to the vocabulary size with a linear layer which would give us a word at each decoding time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = torch.nn.GRU(hidden_size, hidden_size)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim = 2)\n",
    "    \n",
    "    def forward(self, x, s):\n",
    "        embedded = self.embedding(x)\n",
    "        _ , hidden_state = self.gru(embedded, s)\n",
    "        output = self.fc(hidden_state)\n",
    "        return self.softmax(output), hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Two approaches to training. The first one is the slower but more easy to conceptualize than the second so we will start with the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 512\n",
    "LEARNING_RATE = 0.01\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Recursivelly propagate encoder\n",
    "def propagate_encoder(encoder, question, state):\n",
    "    try: encoder_input = next(question).view(1, 1)\n",
    "    except StopIteration: return state\n",
    "    _ , encoder_state = encoder(encoder_input, state)\n",
    "    return propagate_encoder(encoder, question, encoder_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Demonstration\n",
    "Initialize encoder create an iterable from the question initialize state and propagate to get final encoder state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encoder():\n",
    "    unencoded_pairs = process_pairs('data/dialogues/AGREEMENT_BOT.txt')\n",
    "    pairs, dictionary = create_dictionary_encode(unencoded_pairs)\n",
    "    encoder = Encoder(dictionary.__len__(), HIDDEN_SIZE)\n",
    "    question = iter(torch.tensor(pairs[0][0], dtype = torch.long))\n",
    "    initial_hidden_state = encoder.__init_hidden__(HIDDEN_SIZE).repeat(1, 1, 1)\n",
    "    encoding = propagate_encoder(encoder, question, initial_hidden_state)\n",
    "    assert encoding.shape == (1, 1, 512)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = test_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Propagate Decoder to get loss\n",
    "def propagate_decoder(decoder, answer, decoder_input, decoder_state, loss_function, loss = 0):\n",
    "    try: target = next(answer).view(1)\n",
    "    except StopIteration: return loss\n",
    "    decoder_prediction, state = decoder(decoder_input, decoder_state)\n",
    "    sample_loss = loss_function(decoder_prediction.squeeze(0), target)\n",
    "    output = torch.argmax(decoder_prediction, dim = 2).detach()\n",
    "    if output.item() == 1: return loss\n",
    "    return propagate_decoder(decoder, answer, output, state, loss_function, loss + sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_decoder(encoding):\n",
    "    unencoded_pairs = process_pairs('data/dialogues/AGREEMENT_BOT.txt')\n",
    "    pairs, dictionary = create_dictionary_encode(unencoded_pairs)\n",
    "    decoder = Decoder(dictionary.__len__(), HIDDEN_SIZE)\n",
    "    answer = iter(torch.tensor(pairs[0][1], dtype = torch.long))\n",
    "    initial_decoder_token = torch.tensor([[0]])\n",
    "    loss_function = torch.nn.NLLLoss()\n",
    "    loss = propagate_decoder(decoder, answer, initial_decoder_token, encoding, loss_function)\n",
    "    try: loss.backward()\n",
    "    except: print('Failure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_decoder(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_update(loss, encoder_optimizer, decoder_optimizer):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    encoder_optimizer.step() \n",
    "    decoder_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_training_pass(encoder, decoder, pairs, criterion, encoder_optim , decoder_optim):\n",
    "    iteration_loss = 0\n",
    "    for (question, answer) in pairs:\n",
    "        question = iter(torch.tensor(question, dtype = torch.long, device = DEVICE)) \n",
    "        initial_hidden_state = encoder.__init_hidden__(HIDDEN_SIZE).repeat(1, 1, 1).to(DEVICE)\n",
    "        encoding = propagate_encoder(encoder, question, initial_hidden_state)\n",
    "        answer = iter(torch.tensor(answer, dtype = torch.long, device = DEVICE))\n",
    "        initial_decoder_token = torch.tensor([[0]], device = DEVICE)\n",
    "        loss = propagate_decoder(decoder, answer, initial_decoder_token, encoding, criterion)\n",
    "        if loss != 0: train_update(loss, encoder_optim, decoder_optim)\n",
    "        iteration_loss += loss.item()\n",
    "    return iteration_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test(iterations):\n",
    "    unencoded_pairs = process_pairs('data/dialogues/AGREEMENT_BOT.txt')\n",
    "    pairs, dictionary = create_dictionary_encode(unencoded_pairs)\n",
    "    loss_function = torch.nn.NLLLoss()\n",
    "    encoder = Encoder(dictionary.__len__(), HIDDEN_SIZE)\n",
    "    decoder = Decoder(dictionary.__len__(), HIDDEN_SIZE)\n",
    "    encoder = encoder.to(DEVICE)\n",
    "    decoder = decoder.to(DEVICE)\n",
    "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr = LEARNING_RATE)\n",
    "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr = LEARNING_RATE)\n",
    "    optimizers = (encoder_optimizer, decoder_optimizer)\n",
    "    for iteration in range(iterations):\n",
    "        loss = single_training_pass(encoder, decoder, pairs, loss_function, *optimizers)\n",
    "        print(f'Iteration: {iteration}. Loss: {loss}')\n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. Loss: 22486.699201107025\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Encoder(\n",
       "   (embedding): Embedding(1906, 512)\n",
       "   (gru): GRU(512, 512)\n",
       " ), Decoder(\n",
       "   (embedding): Embedding(1906, 512)\n",
       "   (gru): GRU(512, 512)\n",
       "   (fc): Linear(in_features=512, out_features=1906, bias=True)\n",
       "   (softmax): LogSoftmax()\n",
       " ))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(encoder, decoder, encoder_filename, decoder_filename):\n",
    "    torch.save(encoder.state_dict(), encoder_filename)\n",
    "    torch.save(decoder.state_dict(), decoder_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_loop(encoder, decoder, dictionary):\n",
    "    rev = reverse_dictionary(dictionary)\n",
    "    while True:\n",
    "        question = normalize_sentence(input('Q:'))\n",
    "        try: encoded_question = encode_sentence(question, dictionary)\n",
    "        except: \n",
    "            print('I have not seen one of these words before')\n",
    "            continue\n",
    "        query = iter(torch.tensor(encoded_question, dtype = torch.long))\n",
    "        encoder_state = encoder.__init_hidden__(HIDDEN_SIZE).repeat(1, 1, 1)\n",
    "        encoding = propagate_encoder(encoder, query, encoder_state)\n",
    "        decoder_input = torch.tensor([[0]])\n",
    "        decoder_state = encoding\n",
    "        final_decoding = []\n",
    "        while decoder_input.item() != 1:\n",
    "            decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
    "            decoder_input = torch.argmax(decoder_output, dim = 2)\n",
    "            final_decoding += [decoder_input.item()]\n",
    "        print(decode_sentence(final_decoding, rev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel Training\n",
    "This process is very slow and realizing how to parallelize this process in crucial. This is the tutorial from pytorch for sequence to sequence models https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html and offers no parallelization. Most importantly propagation though the encoder and decoder are done manually. If we refer back to the docs we can see than torch.nn.gru can take a whole sequence and automatically propagates state. Therefore propagate_encoder(encoder, question, initial_state) is equivelant to encoder(question, initial_state). Same applies to the decoder. Further we have been using a batch size of one which does not allow for almost any paralelization. It is impossible to parallelize pairs with different dimensions but we can do it ones with the same ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_dimensions = lambda q, a: (len(q), len(a))\n",
    "def get_parallel_pairs(pairs):\n",
    "    dimension_range = range(MIN_LENGTH, MAX_LENGTH + 1)\n",
    "    for dimension in [(ql, al) for ql in dimension_range for al in dimension_range]:\n",
    "        yield [*filter(lambda p: pair_dimensions(*p) == dimension, pairs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is the key to this approach. It takes any dimensions which the pair could take and returns all pairs with exactly that dimension. We need to rewrite the functions which propagate encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "transpose = lambda t: torch.transpose(t, 0, 1)\n",
    "\n",
    "def propagate_encoder(encoder, questions, sequence_size):\n",
    "    questions = transpose(torch.tensor(questions, dtype = torch.long))\n",
    "    initial_encoder_state = encoder.__init_hidden__(HIDDEN_SIZE).repeat(1, sequence_size, 1)\n",
    "    return encoder(questions, initial_encoder_state)\n",
    "\n",
    "def propagate_decoder(decoder, answers, decoder_input, decoder_state, loss_function, loss = 0):\n",
    "    try: target = next(answers)\n",
    "    except StopIteration: return loss\n",
    "    decoder_predictions, state = decoder(decoder_input, decoder_state)\n",
    "    sample_loss = loss_function(decoder_predictions.squeeze(0), target)\n",
    "    output = torch.argmax(decoder_predictions, dim = 2).detach()\n",
    "    return propagate_decoder(decoder, answers, output, state, loss_function, loss + sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_training(encoder, decoder, pairs, encoder_optim, decoder_optim, loss_function):\n",
    "    iteration_loss = 0\n",
    "    for parallel_pairs in get_parallel_pairs(pairs):\n",
    "        if parallel_pairs == []: continue\n",
    "        sequence_size = len(parallel_pairs)\n",
    "        questions, answers = tuple(map(list, zip(*parallel_pairs)))\n",
    "        encoder_outputs, encoder_state = propagate_encoder(encoder, questions, sequence_size)\n",
    "        decoder_input_tokens = torch.tensor([[0]], dtype = torch.long).repeat(1, sequence_size)\n",
    "        decoder_answers = iter(transpose(torch.tensor(answers, dtype = torch.long)))\n",
    "        decoder_args = decoder, decoder_answers, decoder_input_tokens, encoder_state \n",
    "        loss = propagate_decoder(*decoder_args, loss_function)\n",
    "        train_update(loss, encoder_optim, decoder_optim)\n",
    "        iteration_loss += loss.item()\n",
    "    return iteration_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_test(iterations):\n",
    "    unencoded_pairs = process_pairs('data/dialogues/AGREEMENT_BOT.txt')\n",
    "    pairs, dictionary = create_dictionary_encode(unencoded_pairs)\n",
    "    encoder = Encoder(dictionary.__len__(), HIDDEN_SIZE)\n",
    "    decoder = Decoder(dictionary.__len__(), HIDDEN_SIZE)\n",
    "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr = LEARNING_RATE)\n",
    "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr = LEARNING_RATE)\n",
    "    loss_function = torch.nn.NLLLoss()\n",
    "    training_args = encoder, decoder, pairs, encoder_optimizer, decoder_optimizer, loss_function\n",
    "    for i in range(iterations):\n",
    "        loss = parallel_training(*training_args)\n",
    "        print(f'Iteration: {i}. Loss: {loss}')\n",
    "    return encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0. Loss: 5310.17947769165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Encoder(\n",
       "   (embedding): Embedding(1906, 512)\n",
       "   (gru): GRU(512, 512)\n",
       " ), Decoder(\n",
       "   (embedding): Embedding(1906, 512)\n",
       "   (gru): GRU(512, 512)\n",
       "   (fc): Linear(in_features=512, out_features=1906, bias=True)\n",
       "   (softmax): LogSoftmax()\n",
       " ))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_test(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_evaluation_loop(encoder, decoder, dictionary):\n",
    "    rev = reverse_dictionary(dictionary)\n",
    "    while True:\n",
    "        question = normalize_sentence(input('Q:'))\n",
    "        if question == 'quit': return\n",
    "        try: encoded_question = [encode_sentence(question, dictionary)]\n",
    "        except: \n",
    "            print('I have not seen one of these words before')\n",
    "            continue\n",
    "        encoder_state = encoder.__init_hidden__(HIDDEN_SIZE).repeat(1, 1, 1)\n",
    "        _, encoding = propagate_encoder(encoder, encoded_question, 1)\n",
    "        decoder_input = torch.tensor([[0]])\n",
    "        decoder_state = encoding\n",
    "        final_decoding = []\n",
    "        while decoder_input.item() != 1:\n",
    "            decoder_output, decoder_state = decoder(decoder_input, decoder_state)\n",
    "            decoder_input = torch.argmax(decoder_output, dim = 2)\n",
    "            final_decoding += [decoder_input.item()] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "We can add an attention mechanism to our decoder to better model long term dependencies. However, this is unkikely to boost performance significantly since there are no long term dependencies since sequences here are kept short\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder(torch.nn.Module): \n",
    "    def __init__(self, output_size, hidden_size):\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = torch.nn.Linear(2 * hidden_size, MAX_LENGTH)\n",
    "        self.gru = torch.nn.GRU(2 * hidden_size, hidden_size)\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.att_softmax = torch.nn.Softmax(dim = 2)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim = 2)\n",
    "        \n",
    "    def forward(self, x, s, encoder_outputs):\n",
    "        (encoder_length, batch_size, _) = encoder_outputs.shape\n",
    "        embedded = self.embedding(x)\n",
    "        attention_input = torch.cat((embedded, s), dim = 2)\n",
    "        attention_w = self.attention(attention_input)\n",
    "        attention_coefficient = self.att_softmax(attention_w).view(MAX_LENGTH, batch_size, 1)\n",
    "        extended_outputs = torch.zeros(MAX_LENGTH, batch_size, HIDDEN_SIZE)\n",
    "        extended_outputs[:encoder_length] = encoder_outputs\n",
    "        encoder_attention = torch.mul(attention_coefficient, extended_outputs)\n",
    "        collapsed_attention = torch.sum(encoder_attention, dim = 0)\n",
    "        decoder_input = torch.cat((collapsed_attention.repeat(1, 1, 1), embedded), dim = 2)\n",
    "        _, decoder_hidden = self.gru(decoder_input, s)\n",
    "        output = self.fc(decoder_hidden)\n",
    "        return self.softmax(output), decoder_hidden\n",
    "           \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to slightly change our functions to handle the extra decoder input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_adec(decoder, answers, decoder_input, decoder_state, loss_function, outputs, loss = 0):\n",
    "    try: target = next(answers)\n",
    "    except StopIteration: return loss\n",
    "    decoder_predictions, state = decoder(decoder_input, decoder_state, outputs)\n",
    "    sample_loss = loss_function(decoder_predictions.squeeze(0), target)\n",
    "    output = torch.argmax(decoder_predictions, dim = 2).detach()\n",
    "    return prop_adec(decoder, answers, output, state, loss_function, outputs, loss + sample_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_at_training(encoder, decoder, pairs, encoder_optim, decoder_optim, loss_function):\n",
    "    iteration_loss = 0\n",
    "    for parallel_pairs in get_parallel_pairs(pairs):\n",
    "        if parallel_pairs == []: continue\n",
    "        sequence_size = len(parallel_pairs)\n",
    "        questions, answers = tuple(map(list, zip(*parallel_pairs)))\n",
    "        encoder_outputs, encoder_state = propagate_encoder(encoder, questions, sequence_size)\n",
    "        decoder_input_tokens = torch.tensor([[0]], dtype = torch.long).repeat(1, sequence_size)\n",
    "        decoder_answers = iter(transpose(torch.tensor(answers, dtype = torch.long)))\n",
    "        decoder_args = decoder, decoder_answers, decoder_input_tokens, encoder_state \n",
    "        loss = prop_adec(*decoder_args, loss_function, encoder_outputs)\n",
    "        train_update(loss, encoder_optim, decoder_optim)\n",
    "        iteration_loss += loss.item()\n",
    "    return iteration_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attention_parallel(iterations, encoder, decoder, pairs, e_optim, d_optim, loss_f):\n",
    "    g_loss = []\n",
    "    training_args = encoder, decoder, pairs, e_optim, d_optim, loss_f\n",
    "    for i in range(iterations):\n",
    "        loss = parallel_at_training(*training_args)\n",
    "        print(f'Iteration {i}, Loss: {loss}')\n",
    "        g_loss += [loss]\n",
    "    return g_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to change our evaluation loop slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_evaluation_loop(encoder, decoder, dictionary):\n",
    "    rev = reverse_dictionary(dictionary)\n",
    "    while True:\n",
    "        question = normalize_sentence(input('Q:'))\n",
    "        if question == 'quit': return\n",
    "        try: encoded_question = [encode_sentence(question, dictionary)]\n",
    "        except: \n",
    "            print('I have not seen one of these words before')\n",
    "            continue\n",
    "        encoder_state = encoder.__init_hidden__(HIDDEN_SIZE).repeat(1, 1, 1)\n",
    "        encoder_output, encoding = propagate_encoder(encoder, encoded_question, 1)\n",
    "        decoder_input = torch.tensor([[0]])\n",
    "        decoder_state = encoding\n",
    "        final_decoding = []\n",
    "        while decoder_input.item() != 1:\n",
    "            decoder_output, decoder_state = decoder(decoder_input, decoder_state, encoder_output)\n",
    "            decoder_input = torch.argmax(decoder_output, dim = 2)\n",
    "            final_decoding += [decoder_input.item()]\n",
    "        print(decode_sentence(final_decoding, rev))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on the entire dataset is almost impossible without high spec GPU's so I will train on a subset of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_FILES = ['APARTMENT_FINDER.txt', 'CITY_INFO.txt', 'MUSIC_SUGGESTER.txt']\n",
    "FILENAMES = [*map(lambda r: f'data/dialogues/{r}', RAW_FILES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_training_loop(iterations):\n",
    "    unencoded_pairs = FLATTEN(map(process_pairs, FILENAMES))\n",
    "    pairs, dictionary = create_dictionary_encode(unencoded_pairs)\n",
    "    encoder = Encoder(dictionary.__len__(), HIDDEN_SIZE)\n",
    "    decoder = AttentionDecoder(dictionary.__len__(), HIDDEN_SIZE)\n",
    "    encoder_optimizer = torch.optim.SGD(encoder.parameters(), LEARNING_RATE)\n",
    "    decoder_optimizer = torch.optim.SGD(decoder.parameters(), LEARNING_RATE)\n",
    "    loss_function = torch.nn.NLLLoss()\n",
    "    train_args = encoder, decoder, pairs, encoder_optimizer, decoder_optimizer, loss_function\n",
    "    loss = train_attention_parallel(iterations, *train_args)\n",
    "    return encoder, decoder, loss, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 6029.172572135925\n",
      "Iteration 1, Loss: 5205.395483970642\n",
      "Iteration 2, Loss: 5095.528295516968\n",
      "Iteration 3, Loss: 5036.085752487183\n",
      "Iteration 4, Loss: 4933.981091499329\n",
      "Iteration 5, Loss: 4889.173171043396\n"
     ]
    }
   ],
   "source": [
    "encoder, decoder, loss, dictionary = final_training_loop(100)\n",
    "save_models(encoder, decoder, 'models/encoder', 'models/decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd4b365df28>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('Solarize_Light2')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(range(loss.__len__()), loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_evaluation_loop(encoder, decoder, dictionary`)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
